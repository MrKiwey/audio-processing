{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7677853,"sourceType":"datasetVersion","datasetId":4479003},{"sourceId":7698618,"sourceType":"datasetVersion","datasetId":4493660}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Introduction\n\nSystems able to recognize sounds directly from audio recordings are widely applicable. In this project,\nyou’ll attempt to create an audio tagging system by extracting audio clip image representations and then\nusing computer vision-based classification models. You can consider constructing your system using a\nrelatively large-scale competition data set and then evaluate it on its ability to recognize and distinguish more specialized sounds on locally generated recordings.\n\n### Goals\n\n1. Investigate and construct models for automatic audio tagging of noisy recordings.\n2. Adapt this to smaller data sets of audio recordings, either by using a setup motivated by your\nabove findings or by transfer learning.\n3. Construct an audio tagging application.\n\n### Methods and materials\n\nTo achieve Goal 1 of the project, you can, for example, use the FSDKaggle2018 used in the Freesound\nGeneral-Purpose Audio Tagging Challenge on Kaggle. There are 41 categories of audio clips, and the\ngoal is to classify each clip. For the second objective, you can look for a data set on your own or\nconstruct one yourself.\n\nAs part of the project, you should investigate ways to do data augmentation for audio.\nYou’ll make use of a variety of Python audio libraries, e.g., librosa. You should also look into fastxtend, a library built on top of fastai. To construct the application, you’re free to use any solution you know or want to investigate. A natural starting point is the deployment solutions used in the fastai course.\n\nConsider not converting audio to images but instead setting up an audio classification framework that\noperates on audio representations of the data.","metadata":{}},{"cell_type":"markdown","source":"## Resources\n\nThis is the one I used for the code below. I got stuck trying to tranform the labels from string to numerical values for the GPU to be able to use them with the methods below.\nhttps://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\nimport pandas as pd\nfrom pathlib import Path\n\n# Load the labels from the CSV file into a DataFrame\nlabels_csv_path = '/kaggle/input/audio-samples/FSDKaggle2018.meta/FSDKaggle2018.meta/train_post_competition.csv'\ndf_labels = pd.read_csv(labels_csv_path)\n\n# Create a dictionary mapping filenames (without extension) to labels\nlabels_dict = pd.Series(df_labels.label.values, index=df_labels.fname.apply(lambda x: Path(x).stem)).to_dict()\n\n# Define a function to get labels using the dictionary\ndef get_label(file_path):\n    # Extract the filename without extension from the given file path\n    file_stem = Path(file_path).stem\n    # Return the label from the dictionary\n    return labels_dict[file_stem]","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:51:42.851995Z","iopub.execute_input":"2024-03-14T13:51:42.852889Z","iopub.status.idle":"2024-03-14T13:51:42.960030Z","shell.execute_reply.started":"2024-03-14T13:51:42.852857Z","shell.execute_reply":"2024-03-14T13:51:42.958700Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Prepare training data from Metadata file\n# ----------------------------\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read metadata file\ndf = pd.read_csv('/kaggle/input/audio-samples/FSDKaggle2018.meta/FSDKaggle2018.meta/train_post_competition.csv')\ndf.head()\n\n# Take relevant columns\ndf = df[['fname', 'label']]\ndf.head()\n\n# Create a table with numerical labels\n# Convert labels from strings to numerical labels\nlabel_encoder = LabelEncoder()\nnumerical_labels = label_encoder.fit_transform(df[1])\ndf[1] = numerical_labels\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:48:40.140272Z","iopub.execute_input":"2024-03-14T13:48:40.140629Z","iopub.status.idle":"2024-03-14T13:48:40.290190Z","shell.execute_reply.started":"2024-03-14T13:48:40.140600Z","shell.execute_reply":"2024-03-14T13:48:40.288791Z"},"trusted":true},"execution_count":46,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 1","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create a table with numerical labels\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert labels from strings to numerical labels\u001b[39;00m\n\u001b[1;32m     18\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m---> 19\u001b[0m numerical_labels \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     20\u001b[0m df[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_labels\n\u001b[1;32m     22\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 1"],"ename":"KeyError","evalue":"1","output_type":"error"}]},{"cell_type":"code","source":"import math, random\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio\n\nclass AudioUtil():\n  # ----------------------------\n  # Load an audio file. Return the signal as a tensor and the sample rate\n  # ----------------------------\n    @staticmethod\n    def open(audio_file):\n        sig, sr = torchaudio.load(audio_file)\n        return (sig, sr)\n\n\n # ----------------------------\n  # Convert the given audio to the desired number of channels\n  # ----------------------------\n    @staticmethod\n    def rechannel(aud, new_channel):\n        sig, sr = aud\n\n        if (sig.shape[0] == new_channel):\n          # Nothing to do\n          return aud\n\n        if (new_channel == 1):\n          # Convert from stereo to mono by selecting only the first channel\n          resig = sig[:1, :]\n        else:\n          # Convert from mono to stereo by duplicating the first channel\n          resig = torch.cat([sig, sig])\n\n        return ((resig, sr))\n\n\n  # ----------------------------\n  # Since Resample applies to a single channel, we resample one channel at a time\n  # ----------------------------\n    @staticmethod\n    def resample(aud, newsr):\n        sig, sr = aud\n\n        if (sr == newsr):\n          # Nothing to do\n          return aud\n\n        num_channels = sig.shape[0]\n        # Resample first channel\n        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n        if (num_channels > 1):\n            # Resample the second channel and merge both channels\n            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n            resig = torch.cat([resig, retwo])\n\n        return ((resig, newsr))\n\n\n  # ----------------------------\n  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n  # ----------------------------\n    @staticmethod\n    def pad_trunc(aud, max_ms):\n        sig, sr = aud\n        num_rows, sig_len = sig.shape\n        max_len = sr//1000 * max_ms\n\n        if (sig_len > max_len):\n          # Truncate the signal to the given length\n          sig = sig[:,:max_len]\n\n        elif (sig_len < max_len):\n            # Length of padding to add at the beginning and end of the signal\n            pad_begin_len = random.randint(0, max_len - sig_len)\n            pad_end_len = max_len - sig_len - pad_begin_len\n\n            # Pad with 0s\n            pad_begin = torch.zeros((num_rows, pad_begin_len))\n            pad_end = torch.zeros((num_rows, pad_end_len))\n\n            sig = torch.cat((pad_begin, sig, pad_end), 1)\n      \n        return (sig, sr)\n\n\n\n  # ----------------------------\n  # Shifts the signal to the left or right by some percent. Values at the end\n  # are 'wrapped around' to the start of the transformed signal.\n  # ----------------------------\n    @staticmethod\n    def time_shift(aud, shift_limit):\n        sig,sr = aud\n        _, sig_len = sig.shape\n        shift_amt = int(random.random() * shift_limit * sig_len)\n        return (sig.roll(shift_amt), sr)\n\n\n  # ----------------------------\n  # Generate a Spectrogram\n  # ----------------------------\n    @staticmethod\n    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n        sig,sr = aud\n        top_db = 80\n\n        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n\n        # Convert to decibels\n        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n        return (spec)\n\n\n  # ----------------------------\n  # Augment the Spectrogram by masking out some sections of it in both the frequency\n  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n  # overfitting and to help the model generalise better. The masked sections are\n  # replaced with the mean value.\n  # ----------------------------\n    @staticmethod\n    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec\n\n        freq_mask_param = max_mask_pct * n_mels\n        for _ in range(n_freq_masks):\n              aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n\n        time_mask_param = max_mask_pct * n_steps\n        for _ in range(n_time_masks):\n              aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n                \n        return aug_spec","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:34:42.670596Z","iopub.execute_input":"2024-03-14T13:34:42.670951Z","iopub.status.idle":"2024-03-14T13:34:42.690333Z","shell.execute_reply.started":"2024-03-14T13:34:42.670923Z","shell.execute_reply":"2024-03-14T13:34:42.689155Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, random_split\nimport torchaudio\n\n# ----------------------------\n# Sound Dataset\n# ----------------------------\nclass SoundDS(Dataset):\n    def __init__(self, df, data_path):\n        self.df = df\n        self.data_path = str(data_path)\n        self.duration = 4000\n        self.sr = 44100\n        self.channel = 2\n        self.shift_pct = 0.4\n            \n  # ----------------------------\n  # Number of items in dataset\n  # ----------------------------\n    def __len__(self):\n        return len(self.df)    \n    \n  # ----------------------------\n  # Get i'th item in dataset\n  # ----------------------------\n    def __getitem__(self, idx):\n    # Absolute file path of the audio file - concatenate the audio directory with\n    # the relative path\n        audio_file = self.data_path + \"/\" + self.df.loc[idx, 'fname']\n    # Get the Class ID\n        class_id = self.df.loc[idx, 'label']\n\n        aud = AudioUtil.open(audio_file)\n    # Some sounds have a higher sample rate, or fewer channels compared to the\n    # majority. So make all sounds have the same number of channels and same \n    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n    # result in arrays of different lengths, even though the sound duration is\n    # the same.\n        reaud = AudioUtil.resample(aud, self.sr)\n        rechan = AudioUtil.rechannel(reaud, self.channel)\n\n        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n        return aug_sgram, class_id","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:37:28.083030Z","iopub.execute_input":"2024-03-14T13:37:28.083739Z","iopub.status.idle":"2024-03-14T13:37:28.093837Z","shell.execute_reply.started":"2024-03-14T13:37:28.083709Z","shell.execute_reply":"2024-03-14T13:37:28.092644Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\nmyds = SoundDS(df, '/kaggle/input/audio-samples/FSDKaggle2018.audio_train/FSDKaggle2018.audio_train')\n\n# Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n# Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:37:37.374548Z","iopub.execute_input":"2024-03-14T13:37:37.375545Z","iopub.status.idle":"2024-03-14T13:37:37.383072Z","shell.execute_reply.started":"2024-03-14T13:37:37.375511Z","shell.execute_reply":"2024-03-14T13:37:37.382144Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.nn import init\n\n# ----------------------------\n# Audio Classification Model\n# ----------------------------\nclass AudioClassifier (nn.Module):\n    # ----------------------------\n    # Build the model architecture\n    # ----------------------------\n    def __init__(self):\n        super().__init__()\n        conv_layers = []\n\n        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        init.kaiming_normal_(self.conv1.weight, a=0.1)\n        self.conv1.bias.data.zero_()\n        conv_layers += [self.conv1, self.relu1, self.bn1]\n\n        # Second Convolution Block\n        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm2d(16)\n        init.kaiming_normal_(self.conv2.weight, a=0.1)\n        self.conv2.bias.data.zero_()\n        conv_layers += [self.conv2, self.relu2, self.bn2]\n\n        # Third Convolution Block\n        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu3 = nn.ReLU()\n        self.bn3 = nn.BatchNorm2d(32)\n        init.kaiming_normal_(self.conv3.weight, a=0.1)\n        self.conv3.bias.data.zero_()\n        conv_layers += [self.conv3, self.relu3, self.bn3]\n\n        # Fourth Convolution Block\n        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(64)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Linear Classifier\n        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n        self.lin = nn.Linear(in_features=64, out_features=10)\n\n        # Wrap the Convolutional Blocks\n        self.conv = nn.Sequential(*conv_layers)\n \n    # ----------------------------\n    # Forward pass computations\n    # ----------------------------\n    def forward(self, x):\n        # Run the convolutional blocks\n        x = self.conv(x)\n\n        # Adaptive pool and flatten for input to linear layer\n        x = self.ap(x)\n        x = x.view(x.shape[0], -1)\n\n        # Linear layer\n        x = self.lin(x)\n\n        # Final output\n        return x\n\n# Create the model and put it on the GPU if available\nmyModel = AudioClassifier()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = myModel.to(device)\n# Check that it is on Cuda\nnext(myModel.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:37:39.643247Z","iopub.execute_input":"2024-03-14T13:37:39.643717Z","iopub.status.idle":"2024-03-14T13:37:39.675856Z","shell.execute_reply.started":"2024-03-14T13:37:39.643682Z","shell.execute_reply":"2024-03-14T13:37:39.674896Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"# ----------------------------\n# Training Loop\n# ----------------------------\ndef training(model, train_dl, num_epochs):\n  # Loss Function, Optimizer and Scheduler\n  criterion = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n                                                steps_per_epoch=int(len(train_dl)),\n                                                epochs=num_epochs,\n                                                anneal_strategy='linear')\n\n  # Repeat for each epoch\n  for epoch in range(num_epochs):\n    running_loss = 0.0\n    correct_prediction = 0\n    total_prediction = 0\n\n    # Repeat for each batch in the training set\n    for i, data in enumerate(train_dl):\n        # Get the input features and target labels, and put them on the GPU\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # Normalize the inputs\n        inputs_m, inputs_s = inputs.mean(), inputs.std()\n        inputs = (inputs - inputs_m) / inputs_s\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Keep stats for Loss and Accuracy\n        running_loss += loss.item()\n\n        # Get the predicted class with the highest score\n        _, prediction = torch.max(outputs,1)\n        # Count of predictions that matched the target label\n        correct_prediction += (prediction == labels).sum().item()\n        total_prediction += prediction.shape[0]\n\n        #if i % 10 == 0:    # print every 10 mini-batches\n        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n    \n    # Print stats at the end of the epoch\n    num_batches = len(train_dl)\n    avg_loss = running_loss / num_batches\n    acc = correct_prediction/total_prediction\n    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n  print('Finished Training')\n  \nnum_epochs=2   # Just for demo, adjust this higher.\ntraining(myModel, train_dl, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T13:37:43.167130Z","iopub.execute_input":"2024-03-14T13:37:43.167861Z","iopub.status.idle":"2024-03-14T13:37:43.939802Z","shell.execute_reply.started":"2024-03-14T13:37:43.167827Z","shell.execute_reply":"2024-03-14T13:37:43.938544Z"},"trusted":true},"execution_count":43,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m   \u001b[38;5;66;03m# Just for demo, adjust this higher.\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, train_dl, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Repeat for each batch in the training set\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dl):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Get the input features and target labels, and put them on the GPU\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Normalize the inputs\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     inputs_m, inputs_s \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mmean(), inputs\u001b[38;5;241m.\u001b[39mstd()\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"],"ename":"AttributeError","evalue":"'tuple' object has no attribute 'to'","output_type":"error"}]},{"cell_type":"markdown","source":"# Old code below","metadata":{}},{"cell_type":"code","source":"# Define DataBlock\nspectrogram_block = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=get_label,\n    item_tfms=Resize(224)\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:36:44.136512Z","iopub.execute_input":"2024-02-25T15:36:44.137297Z","iopub.status.idle":"2024-02-25T15:36:44.144336Z","shell.execute_reply.started":"2024-02-25T15:36:44.137264Z","shell.execute_reply":"2024-02-25T15:36:44.143257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Point to the path where train images are stored\npath = Path('/kaggle/input/spectogramszip/Spectograms')\n\n# Create the DataLoaders\ndls = spectrogram_block.dataloaders(path, bs=64)\n\n# Create a learner with a pre-trained model\nlearn = vision_learner(dls, resnet34, metrics=accuracy)\n\n# Find an appropriate learning rate\nlearn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:36:49.738135Z","iopub.execute_input":"2024-02-25T15:36:49.738958Z","iopub.status.idle":"2024-02-25T15:37:16.984568Z","shell.execute_reply.started":"2024-02-25T15:36:49.738920Z","shell.execute_reply":"2024-02-25T15:37:16.983436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:49:13.705224Z","iopub.execute_input":"2024-02-25T15:49:13.705724Z","iopub.status.idle":"2024-02-25T15:49:15.210748Z","shell.execute_reply.started":"2024-02-25T15:49:13.705692Z","shell.execute_reply":"2024-02-25T15:49:15.209611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nlearn.fit_one_cycle(5, 0.0014454397605732083)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:42:31.312275Z","iopub.execute_input":"2024-02-25T15:42:31.312947Z","iopub.status.idle":"2024-02-25T15:44:21.212732Z","shell.execute_reply.started":"2024-02-25T15:42:31.312913Z","shell.execute_reply":"2024-02-25T15:44:21.211620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an interpretation object\ninterp = ClassificationInterpretation.from_learner(learn)\n\n# Plot the confusion matrix\ninterp.plot_confusion_matrix(figsize=(12, 12), dpi=60)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:51:22.625067Z","iopub.execute_input":"2024-02-25T15:51:22.626016Z","iopub.status.idle":"2024-02-25T15:51:34.867462Z","shell.execute_reply.started":"2024-02-25T15:51:22.625978Z","shell.execute_reply":"2024-02-25T15:51:34.866308Z"},"trusted":true},"execution_count":null,"outputs":[]}]}