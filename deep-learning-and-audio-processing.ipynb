{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7677853,"sourceType":"datasetVersion","datasetId":4479003},{"sourceId":7698618,"sourceType":"datasetVersion","datasetId":4493660}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Introduction\n\nSystems able to recognize sounds directly from audio recordings are widely applicable. In this project,\nyou’ll attempt to create an audio tagging system by extracting audio clip image representations and then\nusing computer vision-based classification models. You can consider constructing your system using a\nrelatively large-scale competition data set and then evaluate it on its ability to recognize and distinguish more specialized sounds on locally generated recordings.\n\n### Goals\n\n1. Investigate and construct models for automatic audio tagging of noisy recordings.\n2. Adapt this to smaller data sets of audio recordings, either by using a setup motivated by your\nabove findings or by transfer learning.\n3. Construct an audio tagging application.\n\n### Methods and materials\n\nTo achieve Goal 1 of the project, you can, for example, use the FSDKaggle2018 used in the Freesound\nGeneral-Purpose Audio Tagging Challenge on Kaggle. There are 41 categories of audio clips, and the\ngoal is to classify each clip. For the second objective, you can look for a data set on your own or\nconstruct one yourself.\n\nAs part of the project, you should investigate ways to do data augmentation for audio.\nYou’ll make use of a variety of Python audio libraries, e.g., librosa. You should also look into fastxtend, a library built on top of fastai. To construct the application, you’re free to use any solution you know or want to investigate. A natural starting point is the deployment solutions used in the fastai course.\n\nConsider not converting audio to images but instead setting up an audio classification framework that\noperates on audio representations of the data.","metadata":{}},{"cell_type":"markdown","source":"## Resources\n\nThis is the one I used for the code below. I got stuck trying to tranform the labels from string to numerical values for the GPU to be able to use them with the methods below.\nhttps://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\nimport pandas as pd\nfrom pathlib import Path\n\n# Load the labels from the CSV file into a DataFrame\nlabels_csv_path = '/kaggle/input/audio-samples/FSDKaggle2018.meta/FSDKaggle2018.meta/train_post_competition.csv'\ndf_labels = pd.read_csv(labels_csv_path)\n\n# Create a dictionary mapping filenames (without extension) to labels\nlabels_dict = pd.Series(df_labels.label.values, index=df_labels.fname.apply(lambda x: Path(x).stem)).to_dict()\n\n# Define a function to get labels using the dictionary\ndef get_label(file_path):\n    # Extract the filename without extension from the given file path\n    file_stem = Path(file_path).stem\n    # Return the label from the dictionary\n    return labels_dict[file_stem]","metadata":{"execution":{"iopub.status.busy":"2024-03-21T12:01:39.504060Z","iopub.execute_input":"2024-03-21T12:01:39.504447Z","iopub.status.idle":"2024-03-21T12:01:39.742928Z","shell.execute_reply.started":"2024-03-21T12:01:39.504415Z","shell.execute_reply":"2024-03-21T12:01:39.741725Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastai'"],"ename":"ModuleNotFoundError","evalue":"No module named 'fastai'","output_type":"error"}]},{"cell_type":"code","source":"# ----------------------------\n# Prepare training data from Metadata file\n# ----------------------------\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read metadata file\ndf = pd.read_csv('/kaggle/input/audio-samples/FSDKaggle2018.meta/FSDKaggle2018.meta/train_post_competition.csv')\ndf.head()\n\n# Take relevant columns\ndf = df[['fname', 'label']]\ndf.head()\n\n# Create a table with numerical labels\n# Convert labels from strings to numerical labels\nlabel_encoder = LabelEncoder()\nnumerical_labels = label_encoder.fit_transform(df['label'])\ndf['label'] = numerical_labels\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T10:21:27.966762Z","iopub.execute_input":"2024-03-21T10:21:27.967403Z","iopub.status.idle":"2024-03-21T10:21:28.002502Z","shell.execute_reply.started":"2024-03-21T10:21:27.967372Z","shell.execute_reply":"2024-03-21T10:21:28.001647Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"          fname  label\n0  00044347.wav     23\n1  001ca53d.wav     30\n2  002d256b.wav     38\n3  0033e230.wav     19\n4  00353774.wav      6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fname</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00044347.wav</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001ca53d.wav</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>002d256b.wav</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0033e230.wav</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00353774.wav</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import math, random\nimport os\n# Added below code for debugging to find the exact line where the error occurs\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio\n\nclass AudioUtil():\n  # ----------------------------\n  # Load an audio file. Return the signal as a tensor and the sample rate\n  # ----------------------------\n    @staticmethod\n    def open(audio_file):\n        sig, sr = torchaudio.load(audio_file)\n        return (sig, sr)\n\n\n # ----------------------------\n  # Convert the given audio to the desired number of channels\n  # ----------------------------\n    @staticmethod\n    def rechannel(aud, new_channel):\n        sig, sr = aud\n\n        if (sig.shape[0] == new_channel):\n          # Nothing to do\n          return aud\n\n        if (new_channel == 1):\n          # Convert from stereo to mono by selecting only the first channel\n          resig = sig[:1, :]\n        else:\n          # Convert from mono to stereo by duplicating the first channel\n          resig = torch.cat([sig, sig])\n\n        return ((resig, sr))\n\n\n  # ----------------------------\n  # Since Resample applies to a single channel, we resample one channel at a time\n  # ----------------------------\n    @staticmethod\n    def resample(aud, newsr):\n        sig, sr = aud\n\n        if (sr == newsr):\n          # Nothing to do\n          return aud\n\n        num_channels = sig.shape[0]\n        # Resample first channel\n        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n        if (num_channels > 1):\n            # Resample the second channel and merge both channels\n            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n            resig = torch.cat([resig, retwo])\n\n        return ((resig, newsr))\n\n\n  # ----------------------------\n  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n  # ----------------------------\n    @staticmethod\n    def pad_trunc(aud, max_ms):\n        sig, sr = aud\n        num_rows, sig_len = sig.shape\n        max_len = sr//1000 * max_ms\n\n        if (sig_len > max_len):\n          # Truncate the signal to the given length\n          sig = sig[:,:max_len]\n\n        elif (sig_len < max_len):\n            # Length of padding to add at the beginning and end of the signal\n            pad_begin_len = random.randint(0, max_len - sig_len)\n            pad_end_len = max_len - sig_len - pad_begin_len\n\n            # Pad with 0s\n            pad_begin = torch.zeros((num_rows, pad_begin_len))\n            pad_end = torch.zeros((num_rows, pad_end_len))\n\n            sig = torch.cat((pad_begin, sig, pad_end), 1)\n      \n        return (sig, sr)\n\n\n\n  # ----------------------------\n  # Shifts the signal to the left or right by some percent. Values at the end\n  # are 'wrapped around' to the start of the transformed signal.\n  # ----------------------------\n    @staticmethod\n    def time_shift(aud, shift_limit):\n        sig,sr = aud\n        _, sig_len = sig.shape\n        shift_amt = int(random.random() * shift_limit * sig_len)\n        return (sig.roll(shift_amt), sr)\n\n\n  # ----------------------------\n  # Generate a Spectrogram\n  # ----------------------------\n    @staticmethod\n    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n        sig,sr = aud\n        top_db = 80\n\n        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n\n        # Convert to decibels\n        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n        return (spec)\n\n\n  # ----------------------------\n  # Augment the Spectrogram by masking out some sections of it in both the frequency\n  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n  # overfitting and to help the model generalise better. The masked sections are\n  # replaced with the mean value.\n  # ----------------------------\n    @staticmethod\n    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec\n\n        freq_mask_param = max_mask_pct * n_mels\n        for _ in range(n_freq_masks):\n              aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n\n        time_mask_param = max_mask_pct * n_steps\n        for _ in range(n_time_masks):\n              aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n                \n        return aug_spec","metadata":{"execution":{"iopub.status.busy":"2024-03-21T10:21:36.191899Z","iopub.execute_input":"2024-03-21T10:21:36.192762Z","iopub.status.idle":"2024-03-21T10:21:36.809352Z","shell.execute_reply.started":"2024-03-21T10:21:36.192706Z","shell.execute_reply":"2024-03-21T10:21:36.808350Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, random_split\nimport torchaudio\n\n# ----------------------------\n# Sound Dataset\n# ----------------------------\nclass SoundDS(Dataset):\n    def __init__(self, df, data_path):\n        self.df = df\n        self.data_path = str(data_path)\n        self.duration = 4000\n        self.sr = 44100\n        self.channel = 2\n        self.shift_pct = 0.4\n            \n  # ----------------------------\n  # Number of items in dataset\n  # ----------------------------\n    def __len__(self):\n        return len(self.df)    \n    \n  # ----------------------------\n  # Get i'th item in dataset\n  # ----------------------------\n    def __getitem__(self, idx):\n    # Absolute file path of the audio file - concatenate the audio directory with\n    # the relative path\n        audio_file = self.data_path + \"/\" + self.df.loc[idx, 'fname']\n    # Get the Class ID\n        class_id = self.df.loc[idx, 'label']\n\n        aud = AudioUtil.open(audio_file)\n    # Some sounds have a higher sample rate, or fewer channels compared to the\n    # majority. So make all sounds have the same number of channels and same \n    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n    # result in arrays of different lengths, even though the sound duration is\n    # the same.\n        reaud = AudioUtil.resample(aud, self.sr)\n        rechan = AudioUtil.rechannel(reaud, self.channel)\n\n        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n        return aug_sgram, class_id","metadata":{"execution":{"iopub.status.busy":"2024-03-21T10:29:48.002596Z","iopub.execute_input":"2024-03-21T10:29:48.002984Z","iopub.status.idle":"2024-03-21T10:29:48.014059Z","shell.execute_reply.started":"2024-03-21T10:29:48.002953Z","shell.execute_reply":"2024-03-21T10:29:48.013152Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\nmyds = SoundDS(df, '/kaggle/input/audio-samples/FSDKaggle2018.audio_train/FSDKaggle2018.audio_train')\n\n# Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n# Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:15:25.405681Z","iopub.execute_input":"2024-03-21T11:15:25.406575Z","iopub.status.idle":"2024-03-21T11:15:25.413607Z","shell.execute_reply.started":"2024-03-21T11:15:25.406532Z","shell.execute_reply":"2024-03-21T11:15:25.412774Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.nn import init\n\n# ----------------------------\n# Audio Classification Model\n# ----------------------------\nclass AudioClassifier (nn.Module):\n    # ----------------------------\n    # Build the model architecture\n    # ----------------------------\n    def __init__(self):\n        super().__init__()\n        conv_layers = []\n\n        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        init.kaiming_normal_(self.conv1.weight, a=0.1)\n        self.conv1.bias.data.zero_()\n        conv_layers += [self.conv1, self.relu1, self.bn1]\n\n        # Second Convolution Block\n        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm2d(16)\n        init.kaiming_normal_(self.conv2.weight, a=0.1)\n        self.conv2.bias.data.zero_()\n        conv_layers += [self.conv2, self.relu2, self.bn2]\n\n        # Third Convolution Block\n        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu3 = nn.ReLU()\n        self.bn3 = nn.BatchNorm2d(32)\n        init.kaiming_normal_(self.conv3.weight, a=0.1)\n        self.conv3.bias.data.zero_()\n        conv_layers += [self.conv3, self.relu3, self.bn3]\n\n        # Fourth Convolution Block\n        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(64)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Linear Classifier\n        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n        self.lin = nn.Linear(in_features=64, out_features=10)\n\n        # Wrap the Convolutional Blocks\n        self.conv = nn.Sequential(*conv_layers)\n \n    # ----------------------------\n    # Forward pass computations\n    # ----------------------------\n    def forward(self, x):\n        # Run the convolutional blocks\n        x = self.conv(x)\n\n        # Adaptive pool and flatten for input to linear layer\n        x = self.ap(x)\n        x = x.view(x.shape[0], -1)\n\n        # Linear layer\n        x = self.lin(x)\n\n        # Final output\n        return x\n\n# Create the model and put it on the GPU if available\nmyModel = AudioClassifier()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = myModel.to(device)\n# Check that it is on Cuda\nnext(myModel.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:15:28.029965Z","iopub.execute_input":"2024-03-21T11:15:28.030322Z","iopub.status.idle":"2024-03-21T11:15:28.152829Z","shell.execute_reply.started":"2024-03-21T11:15:28.030293Z","shell.execute_reply":"2024-03-21T11:15:28.151655Z"},"trusted":true},"execution_count":30,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m myModel \u001b[38;5;241m=\u001b[39m AudioClassifier()\n\u001b[1;32m     73\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m myModel \u001b[38;5;241m=\u001b[39m \u001b[43mmyModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Check that it is on Cuda\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mnext\u001b[39m(myModel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"markdown","source":"I presume the error lies in loading the data to the GPU, that the tensors are wrong or something?","metadata":{}},{"cell_type":"code","source":"# ----------------------------\n# Training Loop\n# ----------------------------\ndef training(model, train_dl, num_epochs):\n  # Loss Function, Optimizer and Scheduler\n  criterion = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n                                                steps_per_epoch=int(len(train_dl)),\n                                                epochs=num_epochs,\n                                                anneal_strategy='linear')\n\n  # Repeat for each epoch\n  for epoch in range(num_epochs):\n    running_loss = 0.0\n    correct_prediction = 0\n    total_prediction = 0\n\n    # Repeat for each batch in the training set\n    for i, data in enumerate(train_dl):\n        \n        # Get the input features and target labels, and put them on the GPU\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # Normalize the inputs\n        inputs_m, inputs_s = inputs.mean(), inputs.std()\n        inputs = (inputs - inputs_m) / inputs_s\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Keep stats for Loss and Accuracy\n        running_loss += loss.item()\n\n        # Get the predicted class with the highest score\n        _, prediction = torch.max(outputs,1)\n        # Count of predictions that matched the target label\n        correct_prediction += (prediction == labels).sum().item()\n        total_prediction += prediction.shape[0]\n\n        #if i % 10 == 0:    # print every 10 mini-batches\n        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n    \n    # Print stats at the end of the epoch\n    num_batches = len(train_dl)\n    avg_loss = running_loss / num_batches\n    acc = correct_prediction/total_prediction\n    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n  print('Finished Training')\n\n\n# CUDA error: device-side assert triggered - For debugging consider passing CUDA_LAUNCH_BLOCKING=1\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n  \nnum_epochs=2   # Just for demo, adjust this higher.\ntraining(myModel, train_dl, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T11:16:53.684927Z","iopub.execute_input":"2024-03-21T11:16:53.685681Z","iopub.status.idle":"2024-03-21T11:16:54.278251Z","shell.execute_reply.started":"2024-03-21T11:16:53.685648Z","shell.execute_reply":"2024-03-21T11:16:54.277011Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m   \u001b[38;5;66;03m# Just for demo, adjust this higher.\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 23\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, train_dl, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Repeat for each batch in the training set\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dl):\n\u001b[1;32m     21\u001b[0m     \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Get the input features and target labels, and put them on the GPU\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Normalize the inputs\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     inputs_m, inputs_s \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mmean(), inputs\u001b[38;5;241m.\u001b[39mstd()\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"markdown","source":"# Old code below","metadata":{}},{"cell_type":"code","source":"# Define DataBlock\nspectrogram_block = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=get_label,\n    item_tfms=Resize(224)\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:36:44.136512Z","iopub.execute_input":"2024-02-25T15:36:44.137297Z","iopub.status.idle":"2024-02-25T15:36:44.144336Z","shell.execute_reply.started":"2024-02-25T15:36:44.137264Z","shell.execute_reply":"2024-02-25T15:36:44.143257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Point to the path where train images are stored\npath = Path('/kaggle/input/spectogramszip/Spectograms')\n\n# Create the DataLoaders\ndls = spectrogram_block.dataloaders(path, bs=64)\n\n# Create a learner with a pre-trained model\nlearn = vision_learner(dls, resnet34, metrics=accuracy)\n\n# Find an appropriate learning rate\nlearn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:36:49.738135Z","iopub.execute_input":"2024-02-25T15:36:49.738958Z","iopub.status.idle":"2024-02-25T15:37:16.984568Z","shell.execute_reply.started":"2024-02-25T15:36:49.738920Z","shell.execute_reply":"2024-02-25T15:37:16.983436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:49:13.705224Z","iopub.execute_input":"2024-02-25T15:49:13.705724Z","iopub.status.idle":"2024-02-25T15:49:15.210748Z","shell.execute_reply.started":"2024-02-25T15:49:13.705692Z","shell.execute_reply":"2024-02-25T15:49:15.209611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nlearn.fit_one_cycle(5, 0.0014454397605732083)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:42:31.312275Z","iopub.execute_input":"2024-02-25T15:42:31.312947Z","iopub.status.idle":"2024-02-25T15:44:21.212732Z","shell.execute_reply.started":"2024-02-25T15:42:31.312913Z","shell.execute_reply":"2024-02-25T15:44:21.211620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an interpretation object\ninterp = ClassificationInterpretation.from_learner(learn)\n\n# Plot the confusion matrix\ninterp.plot_confusion_matrix(figsize=(12, 12), dpi=60)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:51:22.625067Z","iopub.execute_input":"2024-02-25T15:51:22.626016Z","iopub.status.idle":"2024-02-25T15:51:34.867462Z","shell.execute_reply.started":"2024-02-25T15:51:22.625978Z","shell.execute_reply":"2024-02-25T15:51:34.866308Z"},"trusted":true},"execution_count":null,"outputs":[]}]}